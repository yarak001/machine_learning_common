{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMtKtVOW6MbWE5Afb6ikaEH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yarak001/machine_learning_common/blob/main/TextVectorization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iqqRRZmrDHhD"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_dataset = tf.data.Dataset.from_tensor_slices([\"foo\", \"bar\", \"baz\"])\n",
        "max_features = 5000  # Maximum vocab size.\n",
        "max_len = 10  # Sequence length to pad the outputs to.\n",
        "\n",
        "# Create the layer.\n",
        "vectorize_layer = tf.keras.layers.TextVectorization(\n",
        " max_tokens=max_features,\n",
        " output_mode='int',\n",
        " output_sequence_length=max_len)\n",
        "\n",
        "vectorize_layer = tf.keras.layers.TextVectorization(\n",
        " max_tokens=max_features,\n",
        " output_mode='multi_hot'\n",
        " )\n",
        "\n",
        "# Now that the vocab layer has been created, call `adapt` on the text-only\n",
        "# dataset to create the vocabulary. You don't have to batch, but for large\n",
        "# datasets this means we're not keeping spare copies of the dataset.\n",
        "vectorize_layer.adapt(text_dataset.batch(64))\n",
        "\n",
        "# Create the model that uses the vectorize text layer\n",
        "model = tf.keras.models.Sequential()\n",
        "\n",
        "# Start by creating an explicit input layer. It needs to have a shape of\n",
        "# (1,) (because we need to guarantee that there is exactly one string\n",
        "# input per batch), and the dtype needs to be 'string'.\n",
        "model.add(tf.keras.Input(shape=(1,), dtype=tf.string))\n",
        "\n",
        "# The first layer in our model is the vectorization layer. After this\n",
        "# layer, we have a tensor of shape (batch_size, max_len) containing vocab\n",
        "# indices.\n",
        "model.add(vectorize_layer)\n",
        "\n",
        "# Now, the model can map strings to integers, and you can add an embedding\n",
        "# layer to map these integers to learned embeddings.\n",
        "input_data = [[\"foo qux bar\"], [\"qux baz\"]]\n",
        "model.predict(input_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "Rg_6PvDUDOtx",
        "outputId": "c5d05fe3-2ed7-4d66-c509-dd2ef1931d79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-72150dc0dcb9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m  \u001b[0mmax_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m  \u001b[0moutput_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'multi_hot'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m  \u001b[0moutput_sequence_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m  )\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/layers/preprocessing/text_vectorization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, max_tokens, standardize, split, ngrams, output_mode, output_sequence_length, pad_to_max_tokens, vocabulary, idf_weights, sparse, ragged, **kwargs)\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0moutput_mode\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mINT\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0moutput_sequence_length\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m       raise ValueError(\n\u001b[0;32m--> 314\u001b[0;31m           \u001b[0;34mf\"`output_sequence_length` must not be set if `output_mode` is not \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m           f\"'int'. Received output_sequence_length={output_sequence_length}.\")\n\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: `output_sequence_length` must not be set if `output_mode` is not 'int'. Received output_sequence_length=10."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorize_layer.get_vocabulary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3015GDRuDcwh",
        "outputId": "88577197-66a5-431f-a6ab-76e47a11969a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['', '[UNK]', 'foo', 'baz', 'bar']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_data = [\"earth\", \"wind\", \"and\", \"fire\"]\n",
        "max_len = 50  # Sequence length to pad the outputs to.\n",
        "\n",
        "# Create the layer, passing the vocab directly. You can also pass the\n",
        "# vocabulary arg a path to a file containing one vocabulary word per\n",
        "# line.\n",
        "vectorize_layer = tf.keras.layers.TextVectorization(\n",
        " max_tokens=max_features,\n",
        " output_mode='int',\n",
        " output_sequence_length=max_len,\n",
        " vocabulary=vocab_data)\n",
        "\n",
        "# Because we've passed the vocabulary directly, we don't need to adapt\n",
        "# the layer - the vocabulary is already set. The vocabulary contains the\n",
        "# padding token ('') and OOV token ('[UNK]') as well as the passed tokens.\n",
        "vectorize_layer.get_vocabulary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rFPja-1IDa7L",
        "outputId": "c8a27980-6473-4789-8bfe-188f80221f42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['', '[UNK]', 'earth', 'wind', 'and', 'fire']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcmUJB0H5GV6",
        "outputId": "c1a51007-e62c-4f9d-8c56-00e79d31550e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = [\n",
        "    \"An apparatus and method for networking the keyboard / graphics array monitor / mouse of the multiple computer and managing with the single key board / graphics array monitor / mouse.\",\n",
        "    \"A manufacturing method of プシコース using プシコース epimerization enzyme and this\",\n",
        "    \"* 123 Schizandra &#x000B7; back mixing fermented wine and a manufacturing method thereof.\"\n",
        "]\n",
        "\n",
        "text_dataset = tf.data.Dataset.from_tensor_slices(dataset)"
      ],
      "metadata": {
        "id": "xivyrCW8DZCY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "import sys\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "\n",
        "def clean_doc_tokens(doc):\n",
        "\n",
        "    doc = doc.lower()\n",
        "    tokens = doc.split()\n",
        "    # re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "\n",
        "    # 정규식 pattern 객체 생성\n",
        "    # printable = digits + ascii_letters + punctuation + whitespace\n",
        "    re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
        "\n",
        "    # 구두점 ''로 교체\n",
        "    table = str.maketrans('', '', string.punctuation)\n",
        "    tokens = [word.translate(table) for word in tokens]\n",
        "    # printable이 아닌것들 ''로 교체\n",
        "    tokens_sentence = [re_print.sub('', w) for w in tokens]\n",
        "    # alphabet으로 이뤄진 token만 남김\n",
        "    tokens = [word for word in tokens_sentence if word.isalpha()]\n",
        "\n",
        "    # 불용어(stopword) 제거\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [w for w in tokens if not w in stop_words]\n",
        "    #길이가 2이상이 token만 남김\n",
        "\n",
        "    tokens_keyword = [word for word in tokens if len(word) > 1]\n",
        "\n",
        "    return ' '.join(tokens_sentence), ' '.join(tokens_keyword)\n",
        "\n",
        "for data in dataset:\n",
        "    sentence, keyword = clean_doc_tokens(data)\n",
        "    print(f'sentence: {sentence} \\r\\n keyword: {keyword}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "maacfE_T2wiE",
        "outputId": "891cbc8e-2be3-4646-80b0-5805e8be54e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sentence: an apparatus and method for networking the keyboard  graphics array monitor  mouse of the multiple computer and managing with the single key board  graphics array monitor  mouse \r\n",
            " keyword: apparatus method networking keyboard graphics array monitor mouse multiple computer managing single key board graphics array monitor mouse\n",
            "sentence: a manufacturing method of  using  epimerization enzyme and this \r\n",
            " keyword: manufacturing method using epimerization enzyme\n",
            "sentence:  123 schizandra x000b7 back mixing fermented wine and a manufacturing method thereof \r\n",
            " keyword: schizandra back mixing fermented wine manufacturing method thereof\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TextVectorize Layer의 standardize인자에서 사용\n",
        "# @tf.keras.utils.register_keras_serializable()\n",
        "def sentence_standardization(input_string):\n",
        "    tokens = tf.strings.lower(input_string, encoding='utf-8')  # 소문자로\n",
        "    tokens = tf.strings.regex_replace(tokens, '[%s]' % re.escape(string.punctuation), ' ')  # 구두점 제거\n",
        "    tokens = tf.strings.regex_replace(tokens, '[^%s]' % re.escape(string.printable), ' ')  # printable이 아닌것들 ''로 제거\n",
        "    tokens = tf.strings.regex_replace(tokens, b'\\s+', ' ')  # 연속된 공백을 하나로\n",
        "    tokens = tf.strings.strip(tokens)  # 앞, 뒤 공백 제거\n",
        "    return tokens\n",
        "\n",
        "# TextVectorize Layer의 standardize인자에서 사용\n",
        "# @tf.keras.utils.register_keras_serializable()\n",
        "def keywords_standardization(input_string):\n",
        "    tokens = tf.strings.lower(input_string, encoding='utf-8')  # 소문자로\n",
        "    tokens = tf.strings.regex_replace(tokens, '[%s]' % re.escape(string.punctuation), ' ')  # 구두점 제거\n",
        "    tokens = tf.strings.regex_replace(tokens, '[^%s]' % re.escape(string.printable), ' ')  # printable이 아닌것들 ''로 제거\n",
        "    tokens = tf.strings.regex_replace(tokens, b'\\s+', ' ')  # 연속된 공백을 하나로\n",
        "\n",
        "    tokens = tf.strings.regex_replace(tokens, '[^a-zA-Z]', ' ')  # alphabet이 아닌 것 제거\n",
        "    tokens = tf.strings.regex_replace(tokens, r'\\b(' + r'|'.join(stopwords.words('english')) + r')\\b\\s*', ' ')  # 불용어 제거\n",
        "    tokens = tf.strings.regex_replace(tokens, r'\\b\\w{1}\\b', '')  # 길이가 2미안은 제거\n",
        "    tokens = tf.strings.regex_replace(tokens, b'\\s+', ' ')  # 연속된 공백을 하나로\n",
        "\n",
        "    tokens = tf.strings.strip(tokens)  # 앞, 뒤 공백 제거\n",
        "    return tokens\n"
      ],
      "metadata": {
        "id": "ZIaH1aCI8Nja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for data in dataset:\n",
        "    sentence = sentence_standardization(data)\n",
        "    keyword = keywords_standardization(data)\n",
        "    print(f\"{'*'* 30} \\r\\n origin: {data} \\r\\n sentence: {sentence} \\r\\n keyword: {keyword} \\r\\n{'*'* 30}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81ypLi35pOxE",
        "outputId": "865aaaef-699a-4a2d-92c3-a0b6a3ff93b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "****************************** \r\n",
            " origin: An apparatus and method for networking the keyboard / graphics array monitor / mouse of the multiple computer and managing with the single key board / graphics array monitor / mouse. \r\n",
            " sentence: b'an apparatus and method for networking the keyboard graphics array monitor mouse of the multiple computer and managing with the single key board graphics array monitor mouse' \r\n",
            " keyword: b'apparatus method networking keyboard graphics array monitor mouse multiple computer managing single key board graphics array monitor mouse' \r\n",
            "******************************\n",
            "****************************** \r\n",
            " origin: A manufacturing method of プシコース using プシコース epimerization enzyme and this \r\n",
            " sentence: b'a manufacturing method of using epimerization enzyme and this' \r\n",
            " keyword: b'manufacturing method using epimerization enzyme' \r\n",
            "******************************\n",
            "****************************** \r\n",
            " origin: * 123 Schizandra &#x000B7; back mixing fermented wine and a manufacturing method thereof. \r\n",
            " sentence: b'123 schizandra x000b7 back mixing fermented wine and a manufacturing method thereof' \r\n",
            " keyword: b'schizandra back mixing fermented wine manufacturing method thereof' \r\n",
            "******************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_features = 5000  # Maximum vocab size.\n",
        "max_len = 50  # Sequence length to pad the outputs to.\n",
        "\n",
        "vectorize_layer_int = tf.keras.layers.TextVectorization(\n",
        " standardize = sentence_standardization,\n",
        " max_tokens=max_features,\n",
        " output_mode='int',\n",
        " output_sequence_length=max_len)\n",
        "\n",
        "vectorize_layer_multi_hot = tf.keras.layers.TextVectorization(\n",
        " max_tokens=max_features,\n",
        " output_mode='multi_hot'\n",
        " )"
      ],
      "metadata": {
        "id": "8Xk4YOAU05il"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorize_layer_int.adapt(text_dataset.batch(1))\n",
        "# vectorize_layer_multi_hot.adapt(text_dataset.batch(1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_WZihq-T1Nx9",
        "outputId": "b9b48e4d-c319-49ff-da86-05b596111d1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 13 calls to <function PreprocessingLayer.make_adapt_function.<locals>.adapt_step at 0x7fa4915ca680> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.Input(shape=(1,), dtype=tf.string))\n",
        "model.add(vectorize_layer_int)\n",
        "predict = model.predict(['i love you'])\n",
        "predict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ye2APw1D1Q20",
        "outputId": "e8abd2a3-0fe4-4870-d43d-134fa23e0ebd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer_int_vocab = vectorize_layer_int.get_vocabulary(include_special_tokens=True)\n",
        "vectorizer_int_vocab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-NhDLfO21ez",
        "outputId": "fa7722a5-ea09-45f2-c4ed-a2578ac3e4f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['',\n",
              " '[UNK]',\n",
              " 'and',\n",
              " 'the',\n",
              " 'method',\n",
              " 'of',\n",
              " 'mouse',\n",
              " 'monitor',\n",
              " 'manufacturing',\n",
              " 'graphics',\n",
              " 'array',\n",
              " 'a',\n",
              " 'x000b7',\n",
              " 'with',\n",
              " 'wine',\n",
              " 'using',\n",
              " 'this',\n",
              " 'thereof',\n",
              " 'single',\n",
              " 'schizandra',\n",
              " 'networking',\n",
              " 'multiple',\n",
              " 'mixing',\n",
              " 'managing',\n",
              " 'keyboard',\n",
              " 'key',\n",
              " 'for',\n",
              " 'fermented',\n",
              " 'epimerization',\n",
              " 'enzyme',\n",
              " 'computer',\n",
              " 'board',\n",
              " 'back',\n",
              " 'apparatus',\n",
              " 'an',\n",
              " '123']"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[' '.join([vectorizer_int_vocab[i] for i in item]) for item in predict]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uwfMZQGQ1nBb",
        "outputId": "dce960b5-c088-4f50-f85b-7408d04b1e27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[UNK] [UNK] [UNK]                                               ',\n",
              " '[UNK] [UNK] [UNK]                                               ',\n",
              " '[UNK] [UNK] [UNK]                                               ',\n",
              " '[UNK] [UNK] [UNK]                                               ']"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.strings.regex_replace(\"**121safa** afdasfa 123\", \"\\w*\\d\\w*\", \"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EMJMoHJC5Ph6",
        "outputId": "e92ab7ce-5b8e-46b5-e650-1bcded43c588"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=string, numpy=b'**** afdasfa '>"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "r'|'.join(stopwords.words('english'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "mhbD_nXbAACq",
        "outputId": "852fc6ba-c4a8-482f-d868-e2c2bc8f696f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"i|me|my|myself|we|our|ours|ourselves|you|you're|you've|you'll|you'd|your|yours|yourself|yourselves|he|him|his|himself|she|she's|her|hers|herself|it|it's|its|itself|they|them|their|theirs|themselves|what|which|who|whom|this|that|that'll|these|those|am|is|are|was|were|be|been|being|have|has|had|having|do|does|did|doing|a|an|the|and|but|if|or|because|as|until|while|of|at|by|for|with|about|against|between|into|through|during|before|after|above|below|to|from|up|down|in|out|on|off|over|under|again|further|then|once|here|there|when|where|why|how|all|any|both|each|few|more|most|other|some|such|no|nor|not|only|own|same|so|than|too|very|s|t|can|will|just|don|don't|should|should've|now|d|ll|m|o|re|ve|y|ain|aren|aren't|couldn|couldn't|didn|didn't|doesn|doesn't|hadn|hadn't|hasn|hasn't|haven|haven't|isn|isn't|ma|mightn|mightn't|mustn|mustn't|needn|needn't|shan|shan't|shouldn|shouldn't|wasn|wasn't|weren|weren't|won|won't|wouldn|wouldn't\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import token\n",
        "from IPython.core.displayhook import tokenize\n",
        "from keras_preprocessing.text import Tokenizer\n",
        "\n",
        "dataset = ['l love you.', 'you love me', 'I hate he', 'he loves you']\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_sequences(dataset)\n",
        "\n",
        "print(tokenizer.document_count)\n",
        "print(tokenizer.word_counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-lAQ7HZuZZum",
        "outputId": "ce8c1fa7-68dd-434f-8aa9-0c51bc655d88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4\n",
            "OrderedDict()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gEdjSiRKHB5R"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}